{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "with open('credentials.json') as f:\n",
    "    credentials = json.load(f)\n",
    "\n",
    "ecr_namespace = \"sagemaker-training-containers/\"\n",
    "prefix = \"pyspark-ml\"\n",
    "role = credentials['role']\n",
    "account_id = credentials['account_id']\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "ecr_repository_name = ecr_namespace + prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu-central-1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7f9b0a78f8e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!#sudo docker build -f ./../docker/Dockerfile -t sagemaker-training-containers/basic-training-container ../docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#! bash ./scripts/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855373717346.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-training-containers/pyspark-ml:latest\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = \"{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest\".format(\n",
    "    account_id, region, ecr_repository_name\n",
    ")\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-855373717346/pyspark-ml/train/dummy.csv\n",
      "s3://sagemaker-eu-central-1-855373717346/pyspark-ml/val/dummy.csv\n"
     ]
    }
   ],
   "source": [
    "! echo \"val1, val2, val3\" > dummy.csv\n",
    "print(sagemaker_session.upload_data(\"dummy.csv\", bucket, prefix + \"/train\"))\n",
    "print(sagemaker_session.upload_data(\"dummy.csv\", bucket, prefix + \"/val\"))\n",
    "! rm dummy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Instances Pricing https://aws.amazon.com/sagemaker/pricing/\n",
    "est = sagemaker.estimator.Estimator(\n",
    "    container_image_uri,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    #train_instance_type=\"local\",  # use local mode\n",
    "    train_instance_type='ml.m5.large',\n",
    "    base_job_name=prefix,\n",
    ")\n",
    "\n",
    "est.set_hyperparameters(hp1=\"value1\", hp2=300, hp3=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-07 09:33:31 Starting - Starting the training job...\n",
      "2022-08-07 09:33:54 Starting - Preparing the instances for trainingProfilerReport-1659864799: InProgress\n",
      "......\n",
      "2022-08-07 09:35:35 Downloading - Downloading input data\n",
      "2022-08-07 09:35:35 Training - Downloading the training image..\u001b[34mSetting default log level to \"WARN\".\u001b[0m\n",
      "\u001b[34mTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\u001b[0m\n",
      "\u001b[34m22/08/07 09:35:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\n",
      "2022-08-07 09:36:35 Uploading - Uploading generated training model\n",
      "2022-08-07 09:36:35 Completed - Training job completed\n",
      "\u001b[34mRunning training...\u001b[0m\n",
      "\u001b[34mHyperparameters configuration:\u001b[0m\n",
      "\u001b[34m{'hp1': 'value1', 'hp2': '300', 'hp3': '0.001'}\u001b[0m\n",
      "\u001b[34mInput data configuration:\u001b[0m\n",
      "\u001b[34m{'train': {'ContentType': 'text/csv',\n",
      "           'RecordWrapperType': 'None',\n",
      "           'S3DistributionType': 'FullyReplicated',\n",
      "           'TrainingInputMode': 'File'},\n",
      " 'validation': {'ContentType': 'text/csv',\n",
      "                'RecordWrapperType': 'None',\n",
      "                'S3DistributionType': 'FullyReplicated',\n",
      "                'TrainingInputMode': 'File'}}\u001b[0m\n",
      "\u001b[34mList of files in train channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train/dummy.csv\u001b[0m\n",
      "\u001b[34mList of files in validation channel: \u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/validation/dummy.csv\u001b[0m\n",
      "\u001b[34mResource configuration:\u001b[0m\n",
      "\u001b[34m{'current_group_name': 'homogeneousCluster',\n",
      " 'current_host': 'algo-1',\n",
      " 'current_instance_type': 'ml.m5.large',\n",
      " 'hosts': ['algo-1'],\n",
      " 'instance_groups': [{'hosts': ['algo-1'],\n",
      "                      'instance_group_name': 'homogeneousCluster',\n",
      "                      'instance_type': 'ml.m5.large'}],\n",
      " 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34mTraining job name: \u001b[0m\n",
      "\u001b[34mpyspark-ml-2022-08-07-09-33-07-594\u001b[0m\n",
      "\u001b[34mTraining job ARN: \u001b[0m\n",
      "\u001b[34marn:aws:sagemaker:eu-central-1:855373717346:training-job/pyspark-ml-2022-08-07-09-33-07-594\u001b[0m\n",
      "\u001b[34mCreating Spark Dataframe...\u001b[0m\n",
      "\u001b[34m+---+---+---+\u001b[0m\n",
      "\u001b[34m|  a|  b|  c|\u001b[0m\n",
      "\u001b[34m+---+---+---+\u001b[0m\n",
      "\u001b[34m|  x|  y|  3|\u001b[0m\n",
      "\u001b[34m+---+---+---+\u001b[0m\n",
      "\u001b[34mNone\u001b[0m\n",
      "\u001b[34mRunning epoch 0...\u001b[0m\n",
      "\u001b[34mCompleted epoch 0.\u001b[0m\n",
      "\u001b[34mRunning epoch 1...\u001b[0m\n",
      "\u001b[34mCompleted epoch 1.\u001b[0m\n",
      "\u001b[34mRunning epoch 2...\u001b[0m\n",
      "\u001b[34mCompleted epoch 2.\u001b[0m\n",
      "\u001b[34mRunning epoch 3...\u001b[0m\n",
      "\u001b[34mCompleted epoch 3.\u001b[0m\n",
      "\u001b[34mRunning epoch 4...\u001b[0m\n",
      "\u001b[34mCompleted epoch 4.\u001b[0m\n",
      "\u001b[34mTraining completed!\u001b[0m\n",
      "\u001b[34m#015[Stage 0:>                                                          (0 + 1) / 1]#015#015                                                                                #015\u001b[0m\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n"
     ]
    }
   ],
   "source": [
    "train_config = sagemaker.session.s3_input(\n",
    "    \"s3://{0}/{1}/train/\".format(bucket, prefix), content_type=\"text/csv\"\n",
    ")\n",
    "val_config = sagemaker.session.s3_input(\n",
    "    \"s3://{0}/{1}/val/\".format(bucket, prefix), content_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "est.fit({\"train\": train_config, \"validation\": val_config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cab079a81e46c3dcf6e8c5fa7a8221fdb09c62982c8f1221f7963d8382daa7b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
